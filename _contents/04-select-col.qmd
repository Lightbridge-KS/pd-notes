# Selecting Columns

Column selection is one of the most fundamental operations in data analysis. While R's `dplyr::select()` provides a clean, consistent interface for column selection, pandas offers multiple approaches that can be more flexible but also more confusing initially. This chapter will show you how to achieve tidyverse-style column selection in pandas.


## Best Practices Summary

Here's a quick reference for column selection patterns:

| Task | R (dplyr) | Pandas |
|------|-----------|--------|
| Select columns | `select(df, col1, col2)` | `df[['col1', 'col2']]` |
| Select range | `select(df, col1:col3)` | `df.loc[:, 'col1':'col3']` |
| Contains pattern | `select(df, contains("test"))` | `df.filter(like='test')` |
| Starts with | `select(df, starts_with("test"))` | `df.filter(regex='^test')` |
| Ends with | `select(df, ends_with("test"))` | `df.filter(regex='test$')` |
| By type | `select(df, where(is.numeric))` | `df.select_dtypes(include='number')` |
| Exclude columns | `select(df, -c(col1, col2))` | `df.drop(columns=['col1', 'col2'])` |
| Reorder | `select(df, col2, col1, everything())` | `df[['col2', 'col1'] + other_cols]` |

## Tips for Tidyverse Users

1. **Think in lists**: Most pandas column selection operations expect lists of column names.

2. **Use `.filter()` for patterns**: It's the closest equivalent to tidyselect helpers.

3. **`.loc` for explicit selection**: When you want to be clear about selecting columns (not rows), use `.loc[:, columns]`.

4. **Create helper functions**: Build your own tidyverse-style functions for common patterns.

5. **Chain operations**: Use parentheses for multi-line chains:
   ```python
   result = (df
            .filter(like='score')
            .assign(total=lambda x: x.sum(axis=1)))
   ```

Column selection in pandas is more verbose than dplyr's `select()`, but it's also more flexible. Master these patterns, and you'll find yourself writing cleaner, more maintainable code.

## Basic Column Selection

The simplest forms of column selection in pandas:

```python
import pandas as pd
import numpy as np

# Create sample DataFrame
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
    'age': [25, 30, 35, 28, 33],
    'salary': [70000, 80000, 75000, 72000, 85000],
    'department': ['Sales', 'IT', 'HR', 'Sales', 'IT'],
    'years_experience': [2, 5, 8, 3, 7],
    'performance_rating': [4.2, 4.5, 3.8, 4.0, 4.6]
})

# Single column selection (returns Series)
# R: select(df, name) or df$name
print("Single column (returns Series):")
print(df['name'])
print()

# Multiple columns (returns DataFrame)
# R: select(df, name, age, salary)
print("Multiple columns:")
print(df[['name', 'age', 'salary']])
print()

# Using variables for column names
cols_to_select = ['name', 'department']
print("Columns from variable:")
print(df[cols_to_select])
```

## Column Selection Methods Comparison

Pandas offers several ways to select columns, each with pros and cons:

```python
# Method 1: Bracket notation (most common)
df[['name', 'age']]

# Method 2: .loc accessor (more explicit)
df.loc[:, ['name', 'age']]

# Method 3: .filter() method (regex capable)
df.filter(items=['name', 'age'])

# Method 4: Using column positions with iloc
df.iloc[:, [0, 1]]  # First two columns

# Let's see them in action
print("Bracket notation:")
print(df[['name', 'age']].head(3))
print()

print("Using .loc:")
print(df.loc[:, ['name', 'age']].head(3))
print()

print("Using .filter():")
print(df.filter(items=['name', 'age']).head(3))
```

## Selecting Column Ranges

Unlike R's `select()`, pandas can slice columns if they're in order:

```python
# Column slicing with loc
# R: select(df, name:department)  # if using tidyselect
print("Columns from 'name' to 'department':")
print(df.loc[:, 'name':'department'])
print()

# Using integer positions
# R: select(df, 1:3)
print("Columns by position (first 3):")
print(df.iloc[:, 0:3])  # Note: 0:3 means columns 0, 1, 2
print()

# From column to end
print("From 'salary' to end:")
print(df.loc[:, 'salary':])
```

## Pattern-Based Selection

One area where pandas shines is pattern-based column selection:

```python
# Create DataFrame with patterned column names
df_wide = pd.DataFrame({
    'id': [1, 2, 3],
    'test_score_math': [85, 92, 78],
    'test_score_english': [88, 85, 92],
    'test_score_science': [90, 88, 85],
    'final_grade_math': [87, 93, 80],
    'final_grade_english': [89, 86, 93],
    'student_name': ['Alice', 'Bob', 'Charlie']
})

# Select columns containing 'test'
# R: select(df, contains("test"))
print("Columns containing 'test':")
print(df_wide.filter(like='test'))
print()

# Select columns starting with 'test'
# R: select(df, starts_with("test"))
print("Columns starting with 'test':")
print(df_wide.filter(regex='^test'))
print()

# Select columns ending with 'math'
# R: select(df, ends_with("math"))
print("Columns ending with 'math':")
print(df_wide.filter(regex='math$'))
print()

# Complex regex pattern
print("All score columns:")
print(df_wide.filter(regex='score|grade'))
```

## Selecting by Data Type

Selecting columns based on their data type is very useful:

```python
# DataFrame with mixed types
df_mixed = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salary': [70000.5, 80000.0, 75000.25],
    'is_manager': [False, True, False],
    'department': ['Sales', 'IT', 'HR'],
    'hire_date': pd.to_datetime(['2020-01-15', '2019-03-22', '2018-07-01']),
    'employee_id': [1001, 1002, 1003]
})

# Select numeric columns
# R: select(df, where(is.numeric))
print("Numeric columns only:")
print(df_mixed.select_dtypes(include='number'))
print()

# Select string columns
print("String columns:")
print(df_mixed.select_dtypes(include='object'))
print()

# Select multiple types
print("Numeric and boolean columns:")
print(df_mixed.select_dtypes(include=['number', 'bool']))
print()

# Exclude certain types
print("All except datetime:")
print(df_mixed.select_dtypes(exclude='datetime'))
```

## Creating Tidyverse-Style Helper Functions

To make column selection more tidyverse-like, we can create helper functions:

```python
def select(df, *args):
    """Select columns tidyverse-style"""
    if len(args) == 1 and isinstance(args[0], list):
        return df[args[0]]
    return df[list(args)]

def select_contains(df, pattern):
    """Select columns containing pattern"""
    return df.filter(like=pattern)

def select_starts_with(df, prefix):
    """Select columns starting with prefix"""
    return df.filter(regex=f'^{prefix}')

def select_ends_with(df, suffix):
    """Select columns ending with suffix"""
    return df.filter(regex=f'{suffix}$')

def select_matches(df, pattern):
    """Select columns matching regex pattern"""
    return df.filter(regex=pattern)

# Usage examples
print("Using select() helper:")
print(select(df, 'name', 'age', 'salary'))
print()

print("Using select_contains():")
print(select_contains(df_wide, 'score'))
```

## Excluding Columns

Sometimes it's easier to specify what you don't want:

```python
# Exclude specific columns
# R: select(df, -c(age, salary))
print("All except age and salary:")
print(df.drop(columns=['age', 'salary']))
print()

# Alternative approach
all_cols = df.columns.tolist()
exclude = ['age', 'salary']
keep_cols = [col for col in all_cols if col not in exclude]
print("Using list comprehension:")
print(df[keep_cols])
print()

# Exclude by pattern
# R: select(df, -contains("rating"))
print("Exclude columns with 'rating':")
cols_without_rating = df.columns[~df.columns.str.contains('rating')]
print(df[cols_without_rating])
```

## Reordering Columns

Selecting columns in a specific order:

```python
# Reorder columns
# R: select(df, department, name, everything())
print("Reordered columns:")
print(df[['department', 'name', 'age', 'salary', 'years_experience', 'performance_rating']])
print()

# Move specific columns to front
def move_to_front(df, cols):
    """Move specified columns to front"""
    cols = [cols] if isinstance(cols, str) else cols
    other_cols = [c for c in df.columns if c not in cols]
    return df[cols + other_cols]

print("Department and name first:")
print(move_to_front(df, ['department', 'name']))
print()

# Sort columns alphabetically
print("Alphabetically sorted columns:")
print(df[sorted(df.columns)])
```

## Renaming While Selecting

Combining selection with renaming:

```python
# Select and rename in one step
# R: select(df, employee = name, dept = department)
print("Select and rename:")
print(df[['name', 'department']].rename(columns={
    'name': 'employee',
    'department': 'dept'
}))
print()

# More complex renaming with selection
print("Select numeric columns and add prefix:")
numeric_df = df.select_dtypes(include='number')
numeric_df.columns = 'num_' + numeric_df.columns
print(numeric_df)
```

## Column Selection in Method Chains

Integrating column selection into pandas method chains:

```python
# Create a larger dataset
np.random.seed(42)
df_large = pd.DataFrame({
    'employee_id': range(1000, 1020),
    'first_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'] * 4,
    'last_name': ['Smith', 'Jones', 'Wilson', 'Brown', 'Davis'] * 4,
    'age': np.random.randint(25, 55, 20),
    'salary_base': np.random.randint(50000, 90000, 20),
    'salary_bonus': np.random.randint(5000, 15000, 20),
    'dept_code': np.random.choice(['S', 'I', 'H', 'F'], 20),
    'dept_name': np.random.choice(['Sales', 'IT', 'HR', 'Finance'], 20),
    'years_exp': np.random.randint(1, 15, 20),
    'perf_q1': np.random.uniform(3, 5, 20),
    'perf_q2': np.random.uniform(3, 5, 20),
    'perf_q3': np.random.uniform(3, 5, 20),
    'perf_q4': np.random.uniform(3, 5, 20)
})

# Chain with column selection
# R: df %>% select(contains("name"), starts_with("salary")) %>% head()
result = (df_large
          .filter(regex='name|salary')
          .head())
print("Chained selection:")
print(result)
print()

# More complex chain
# R: df %>% 
#     select(employee_id, first_name, last_name, starts_with("perf")) %>%
#     assign(perf_avg = rowMeans(select(., starts_with("perf"))))
result = (df_large
          .filter(regex='^(employee_id|first_name|last_name|perf)')
          .assign(perf_avg=lambda x: x.filter(like='perf').mean(axis=1))
          .round(2)
          .head())
print("Complex chain with selection:")
print(result)
```

## Advanced Column Selection Patterns

Some advanced patterns for complex selections:

```python
# Select columns based on a condition
# Get columns where any value is greater than 100
high_value_cols = [col for col in df.select_dtypes(include='number').columns 
                   if (df[col] > 100).any()]
print("Columns with values > 100:")
print(df[high_value_cols])
print()

# Select columns with low missing data
# Create sample with missing data
df_missing = df_large.copy()
df_missing.loc[0:5, 'age'] = np.nan
df_missing.loc[3:8, 'salary_base'] = np.nan

# Select columns with less than 10% missing
missing_pct = df_missing.isna().sum() / len(df_missing)
low_missing_cols = missing_pct[missing_pct < 0.1].index.tolist()
print("Columns with <10% missing:")
print(df_missing[low_missing_cols].head())
```

## Performance Considerations

Different selection methods have different performance implications:

```python
# Create large DataFrame for timing
large_df = pd.DataFrame(
    np.random.randn(10000, 100), 
    columns=[f'col_{i}' for i in range(100)]
)

# Compare selection methods
import time

# Method 1: Bracket notation
start = time.time()
for _ in range(1000):
    _ = large_df[['col_0', 'col_10', 'col_20']]
print(f"Bracket notation: {time.time() - start:.4f} seconds")

# Method 2: .loc
start = time.time()
for _ in range(1000):
    _ = large_df.loc[:, ['col_0', 'col_10', 'col_20']]
print(f".loc accessor: {time.time() - start:.4f} seconds")

# Method 3: .filter()
start = time.time()
for _ in range(1000):
    _ = large_df.filter(items=['col_0', 'col_10', 'col_20'])
print(f".filter() method: {time.time() - start:.4f} seconds")
```
